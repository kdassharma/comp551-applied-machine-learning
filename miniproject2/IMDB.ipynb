{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAndCombine(directoryNeg, directoryPos, stopping_words):\n",
    "    outputX = [] # Temporary list for list of reviews\n",
    "    outputY = [] # Temporary list for for list of ratings\n",
    "    \n",
    "    for filename in os.listdir(directoryNeg): # Goes through every file in the directory\n",
    "        with open(''+directoryNeg+'/'+filename, 'r') as file:\n",
    "            data = file.read().replace('\\n', '') # Gets rid of any \\n keywords\n",
    "            data = data.replace('<br />','') # Gets rid of any break tags\n",
    "            data = re.sub(r'[^A-Za-z0-9 ]+', '', data) # Gets rid of any non-alphanumerics but not spaces\n",
    "            for word in stopping_words: # Gets rid of all stopping words\n",
    "                if len(word) < 5: # So that words like 'is' are not removed from within bigger words\n",
    "                    continue\n",
    "                if word in data: \n",
    "                    data = data.replace(word,'')\n",
    "            outputX.append(data)\n",
    "            outputY.append(0)\n",
    "    for filename in os.listdir(directoryPos): # Doing the same as above but for the positive classes\n",
    "        with open(''+directoryPos+'/'+filename, 'r') as file:\n",
    "            data = file.read().replace('\\n', '')\n",
    "            data = data.replace('<br />','')\n",
    "            data = re.sub(r'[^A-Za-z0-9 ]+', '', data)\n",
    "            for word in stopping_words:\n",
    "                if len(word) < 5:\n",
    "                    continue\n",
    "                if word in data:\n",
    "                    data = data.replace(word,'')\n",
    "            outputX.append(data)\n",
    "            outputY.append(1)\n",
    "            \n",
    "    return outputX,outputY\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAndCombineTest(directoryNeg, directoryPos): # This function does the same as above, but for the test \n",
    "    outputX = [] # data instead. Nothing is pre-processed aside from the removal of \\n in order to get file.read() into a string.\n",
    "    outputY = []\n",
    "    for filename in os.listdir(directoryNeg):\n",
    "        with open(''+directoryNeg+'/'+filename, 'r') as file:\n",
    "            data = file.read().replace('\\n', '')\n",
    "            outputX.append(data)\n",
    "            outputY.append(0)\n",
    "    for filename in os.listdir(directoryPos):\n",
    "        with open(''+directoryPos+'/'+filename, 'r') as file:\n",
    "            data = file.read().replace('\\n', '')\n",
    "            outputX.append(data)\n",
    "            outputY.append(1)\n",
    "            \n",
    "    return outputX,outputY\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer): # This class was tested in order to remove stems, but yielded lower \n",
    "    def build_analyzer(self): # accuracies, and are hence not used for preprocessing.\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer  = CountVectorizer() # Initialising the vectoriser for the datasets \n",
    "\n",
    "stopping_words = []\n",
    "with open('./function_words.txt', 'r') as f: # This adds the function words from the text file into a list\n",
    "    for line in f:\n",
    "        no_numbers = ''+re.sub('\\d', '', line) \n",
    "        stopping_words.append(no_numbers.strip())\n",
    "stopping_words = sorted(stopping_words, key=len, reverse=True) # Reverses the list in order to not get rid of smaller\n",
    "# substrings containing stopping words\n",
    "\n",
    "list_of_reviews , list_of_ratings = filterAndCombine('./aclImdb/train/neg',\n",
    "                                                     './aclImdb/train/pos', \n",
    "                                                     stopping_words)\n",
    "    \n",
    "X_train = vectorizer.fit_transform(list_of_reviews) # Using the vectoriser to fit and transform the X values \n",
    "#to the frequency features\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train) # Using the transformer to fit and transform the \n",
    "# vectorised X values to frequency of words in all the documents\n",
    "\n",
    "Y_train = np.asarray(list_of_ratings, dtype=np.int32) # Converts the list into a numpy array\n",
    "\n",
    "# The following does the same as above for the test dataset\n",
    "list_of_reviews_test , list_of_ratings_test = filterAndCombineTest('./aclImdb/test/neg',\n",
    "                                                              './aclImdb/test/pos') \n",
    "X_test = vectorizer.transform(list_of_reviews_test)\n",
    "\n",
    "tfidf_transformer_test = TfidfTransformer()\n",
    "X_test_tfidf = tfidf_transformer_test.fit_transform(X_test)\n",
    "Y_test = np.asarray(list_of_ratings_test, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model approach\n",
    "- The same process is used for every single model\n",
    "- First grid search is used with the relevant parameters for each model to find the best parameters (with 5-cross validation)\n",
    "- Using these values, the model is tested with the held-out test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "              'alpha': (1e-1,1e-2, 1e-3,1e-4,1),\n",
    "              'fit_prior': (True,False)\n",
    "             }\n",
    "gs_clf = GridSearchCV(MultinomialNB(), parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train_tfidf, Y_train)\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_NB = MultinomialNB(alpha = 1,fit_prior = True).fit(X_train_tfidf, Y_train)\n",
    "predicted_NB = clf_NB.predict(X_test_tfidf)\n",
    "np.mean(predicted_NB == Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "              'alpha': (1e-1,1e-2, 1e-3,1e-4,1),\n",
    "              'max_iter': (10,100,1000,10000),\n",
    "              'penalty': ('l2', 'l1', 'elasticnet') \n",
    "             }\n",
    "gs_clf = GridSearchCV(SGDClassifier(), parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train_tfidf, Y_train)\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_NB = SGDClassifier(alpha = 0.0001,penalty = 'l2',max_iter=10).fit(X_train_tfidf, Y_train)\n",
    "predicted_NB = clf_NB.predict(X_test_tfidf)\n",
    "np.mean(predicted_NB == Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "              'ccp_alpha' : (0.1,0.0,1.0)\n",
    "             }\n",
    "\n",
    "gs_clf = GridSearchCV(DecisionTreeClassifier(), parameters, n_jobs=20,verbose=10)\n",
    "gs_clf = gs_clf.fit(X_train_tfidf, Y_train)\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_DT = DecisionTreeClassifier().fit(X_train_tfidf, Y_train)\n",
    "predicted_DT = clf_DT.predict(X_test_tfidf)\n",
    "np.mean(predicted_DT == Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_iter' : (100,1000)\n",
    "             }\n",
    "gs_clf = GridSearchCV(LinearSVC(), parameters, n_jobs=-1,verbose=10)\n",
    "gs_clf = gs_clf.fit(X_train_tfidf, Y_train)\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_SVM = LinearSVC(random_state=10,max_iter=100).fit(X_train_tfidf, Y_train)\n",
    "predicted_SVM = clf_SVM.predict(X_test_tfidf)\n",
    "np.mean(predicted_SVM == Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators':(100,200,300,400,500),\n",
    "    'learning_rate': (5e-1,6e-1,7e-1,8e-1,9e-1,1)\n",
    "             }\n",
    "gs_clf = GridSearchCV(AdaBoostClassifier(), parameters, n_jobs=-1,verbose=10)\n",
    "gs_clf = gs_clf.fit(X_train_tfidf, Y_train)\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_AB = AdaBoostClassifier(n_estimators=300, learning_rate=0.7).fit(X_train_tfidf, Y_train)\n",
    "#clf_AB = AdaBoostClassifier().fit(X_train_tfidf, Y_train)\n",
    "predicted_AB = clf_AB.predict(X_test_tfidf)\n",
    "np.mean(predicted_AB == Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_estimators':(10,100)\n",
    "             }\n",
    "gs_clf = GridSearchCV(RandomForestClassifier(), parameters, n_jobs=-1,verbose=10)\n",
    "gs_clf = gs_clf.fit(X_train_tfidf, Y_train)\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=100).fit(X_train_tfidf, Y_train)\n",
    "predicted_RF = clf_RF.predict(X_test_tfidf)\n",
    "np.mean(predicted_RF == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
