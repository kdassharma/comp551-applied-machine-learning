{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAndCombine(directoryNeg, directoryPos, stopping_words):\n",
    "    outputX = []\n",
    "    outputY = []\n",
    "    for filename in os.listdir(directoryNeg):\n",
    "        with open(''+directoryNeg+'/'+filename, 'r') as file:\n",
    "            data = file.read().replace('\\n', '')\n",
    "            data = data.replace('<br />','')\n",
    "            data = re.sub(r'[^A-Za-z0-9 ]+', '', data)\n",
    "#             data = \" \".join(w for w in nltk.wordpunct_tokenize(data) \\\n",
    "#                  if w.lower() in words or not w.isalpha())\n",
    "            \n",
    "#             for word in stopping_words:\n",
    "#                 if len(word) < 5:\n",
    "#                     continue\n",
    "#                 if word in data:\n",
    "#                     data = data.replace(word,'')\n",
    "            outputX.append(data)\n",
    "            outputY.append(0)\n",
    "    for filename in os.listdir(directoryPos):\n",
    "        with open(''+directoryPos+'/'+filename, 'r') as file:\n",
    "            data = file.read().replace('\\n', '')\n",
    "            data = data.replace('<br />','')\n",
    "            data = re.sub(r'[^A-Za-z0-9 ]+', '', data)\n",
    "\n",
    "#             for word in stopping_words:\n",
    "#                 if len(word) < 5:\n",
    "#                     continue\n",
    "#                 if word in data:\n",
    "#                     data = data.replace(word,'')\n",
    "            outputX.append(data)\n",
    "            outputY.append(1)\n",
    "            \n",
    "    return outputX,outputY\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAndCombineTest(directoryNeg, directoryPos):\n",
    "    outputX = []\n",
    "    outputY = []\n",
    "    for filename in os.listdir(directoryNeg):\n",
    "        with open(''+directoryNeg+'/'+filename, 'r') as file:\n",
    "            data = file.read().replace('\\n', '')\n",
    "            outputX.append(data)\n",
    "            outputY.append(0)\n",
    "    for filename in os.listdir(directoryPos):\n",
    "        with open(''+directoryPos+'/'+filename, 'r') as file:\n",
    "            data = file.read().replace('\\n', '')\n",
    "            outputX.append(data)\n",
    "            outputY.append(1)\n",
    "            \n",
    "    return outputX,outputY\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase = True, stop_words = 'english')\n",
    "# vectorizer = CountVectorizer()\n",
    "# stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "# vectorizer = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "stopping_words = []\n",
    "with open('./function_words.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        no_numbers = ''+re.sub('\\d', '', line) \n",
    "        stopping_words.append(no_numbers.strip())\n",
    "stopping_words = sorted(stopping_words, key=len, reverse=True)\n",
    "\n",
    "list_of_reviews , list_of_ratings = filterAndCombine('/Users/kdassharma1/Documents/GitHub/aclImdb/train/neg',\n",
    "                                                     '/Users/kdassharma1/Documents/GitHub/aclImdb/train/pos', \n",
    "                                                     stopping_words)\n",
    "    \n",
    "X_train = vectorizer.fit_transform(list_of_reviews)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
    "Y_train = np.asarray(list_of_ratings, dtype=np.int32)\n",
    "\n",
    "list_of_reviews_test , list_of_ratings_test = filterAndCombineTest('/Users/kdassharma1/Documents/GitHub/aclImdb/test/neg',\n",
    "                                                              '/Users/kdassharma1/Documents/GitHub/aclImdb/test/pos')\n",
    "X_test = vectorizer.transform(list_of_reviews_test)\n",
    "\n",
    "tfidf_transformer_test = TfidfTransformer()\n",
    "X_test_tfidf = tfidf_transformer_test.fit_transform(X_test)\n",
    "Y_test = np.asarray(list_of_ratings_test, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stopping_words)\n",
    "# print(list_of_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "#                       ('tfidf', TfidfTransformer()),\n",
    "#                       ('clf', MultinomialNB()),\n",
    "#                     ])\n",
    "# text_clf = text_clf.fit(list_of_reviews, list_of_ratings)\n",
    "# parameters = {\n",
    "#               'vect__ngram_range': [(1, 1), (1, 2),(2,2)],\n",
    "#               'tfidf__use_idf': (True,False),\n",
    "#               'clf__alpha': (1e-1,1e-2, 1e-3,1e-4,1),\n",
    "#               'clf__fit_prior': (True,False)\n",
    "#              }\n",
    "\n",
    "# gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "# gs_clf = gs_clf.fit(list_of_reviews, list_of_ratings)\n",
    "# print(gs_clf.best_score_)\n",
    "# print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB().get_params().keys()\n",
    "parameters = {\n",
    "              'alpha': (1e-1,1e-2, 1e-3,1e-4,1),\n",
    "              'fit_prior': (True,False)\n",
    "             }\n",
    "gs_clf = GridSearchCV(MultinomialNB(), parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train_tfidf, Y_train)\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_NB = MultinomialNB(alpha = 1,fit_prior = True).fit(X_train_tfidf, Y_train)\n",
    "predicted_NB = clf_NB.predict(X_test_tfidf)\n",
    "np.mean(predicted_NB == Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "              'alpha': (1e-1,1e-2, 1e-3,1e-4,1),\n",
    "              'max_iter': (10,100,1000,10000),\n",
    "              'penalty': ('l2', 'l1', 'elasticnet')\n",
    "             }\n",
    "gs_clf = GridSearchCV(SGDClassifier(), parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train_tfidf, Y_train)\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_NB = SGDClassifier(alpha = 0.0001,penalty = 'l2').fit(X_train_tfidf, Y_train)\n",
    "predicted_NB = clf_NB.predict(X_test_tfidf)\n",
    "np.mean(predicted_NB == Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "              'criterion': ('gini', 'entropy'),\n",
    "              'splitter': ('best', 'random'),\n",
    "              'max_depth' : (10,20,30,40,50),\n",
    "#               'max_depth' : (10,20,30,40,50,100,150,200,250,300,350,400,450,500)\n",
    "#               'min_samples_split' : (2,4,6,8,10),\n",
    "#               'min_samples_leaf' : (1,2,3,4,5),\n",
    "#               'ccp_alpha' : ()\n",
    "             }\n",
    "gs_clf = GridSearchCV(DecisionTreeClassifier(), parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train_tfidf, Y_train)\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_DT = DecisionTreeClassifier(random_state=10).fit(X_train_tfidf, Y_train)\n",
    "predicted_DT = clf_DT.predict(X_test_tfidf)\n",
    "np.mean(predicted_DT == Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_SVM = LinearSVC(random_state=10).fit(X_train_tfidf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_SVM = clf_SVM.predict(X_test_tfidf)\n",
    "np.mean(predicted_SVM == Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_AB = AdaBoostClassifier(random_state=10).fit(X_train_tfidf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_AB = clf_AB.predict(X_test_tfidf)\n",
    "np.mean(predicted_AB == Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(random_state=10).fit(X_train_tfidf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_RF = clf_RF.predict(X_test_tfidf)\n",
    "np.mean(predicted_RF == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping_words = []\n",
    "# with open('./function_words.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         no_numbers = ''+re.sub('\\d', '', line) \n",
    "#         stopping_words.append(no_numbers.strip())\n",
    "# stopping_words = sorted(stopping_words, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = \"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\"\n",
    "# data = data.replace('<br />','')\n",
    "# #             data = data.replace('.',' ')\n",
    "# #             data = data.replace(',',' ')\n",
    "# data = re.sub(r'[^A-Za-z0-9 ]+', ' ', data)\n",
    "\n",
    "# # d.check(\"Helo\") # returns true or false\n",
    "# # d.suggest(\"Helo\") # returns a suggested spell check\n",
    "\n",
    "# for word in stopping_words:\n",
    "#     if len(word) < 3:\n",
    "#         continue\n",
    "#     if word in data:\n",
    "#         data = data.replace(word,'')\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
