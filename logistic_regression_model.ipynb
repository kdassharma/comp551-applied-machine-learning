{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.style as style\n",
    "\n",
    "# Running all the pre-processing libraries as modules\n",
    "%run Preprocessing_BCW_Data.ipynb\n",
    "%run Preprocessing_Adult_Data.ipynb\n",
    "%run Preprocessing_Ionosphere_Data.ipynb\n",
    "%run preprocessing_mpg_dataset.ipynb\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldCrossVal(k, X, y, rate, iter):\n",
    "    accuracy = 0 # Temporary accuracy variable to be compared for different fold accuracies during validation\n",
    "    model = LogisticRegression() # Calling the logistic regression class to instantiate a model\n",
    "    \n",
    "    size = (int)(len(y)/k)\n",
    "    for i in range(k):\n",
    "        train_x = X[np.r_[0:size*i, size*(i+1):]] # Splitting the x and y train set according the k-fold\n",
    "        train_y = y[np.r_[0:size*i, size*(i+1):]] \n",
    "        test_x = X[(i*size):size*(i+1)] # Splitting the x and y test set according the k-fold\n",
    "        test_y = y[(i*size):size*(i+1)]\n",
    "        # Fitting the model using the train_x features to the train y output\n",
    "        (cost_history, optimal_parameters) = lr.fit(train_x, train_y, params, rate, iter) \n",
    "        # Using the previously fitted model to predict classification for test features\n",
    "        y_pred = lr.predict(test_x, optimal_parameters)\n",
    "        # Evaluating the percentage of classifications which are correct\n",
    "        run_accuracy = lr.evaluate_acc(test_y, y_pred)\n",
    "        # Adding for each fold\n",
    "        accuracy = accuracy + run_accuracy\n",
    "    # Returning the average accuracy for each fold\n",
    "    return accuracy/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    # This function defines the sigmoid function which maps the predicted classifications\n",
    "    # into probabilties between 0 and 1.\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Applying sigmoid function to the weighted sum of the parameters.\n",
    "    def find_cost(self,X, y, weights):\n",
    "        squashed = self.sigmoid(X @ weights)\n",
    "        cost = (1/len(y))*(((-y).T @ np.log(squashed + 1e-5))-((1-y).T @ np.log(1-squashed + 1e-5)))\n",
    "        return cost\n",
    "\n",
    "    # Running full-batch gradient descent for optimisation\n",
    "    # which gives the cost which will have one local minimum during the optimization\n",
    "    # based on the hyperparameters: learning rate and iterations\n",
    "    def fit(self, X, y, params, learning_rate, iterations):\n",
    "        cost_history = np.zeros((iterations,1))\n",
    "        for i in range(iterations):\n",
    "            params = params - (learning_rate/len(y)) * (X.T @ (self.sigmoid(X @ params) - y)) \n",
    "            cost_history[i] = self.find_cost(X, y, params)\n",
    "\n",
    "        return (cost_history, params)\n",
    "\n",
    "    # Predicts the classification of a set of features based on fit, outputs a probability and \n",
    "    # rounds to 0 or 1\n",
    "    def predict(self, X, params):\n",
    "        return np.round(self.sigmoid(X @ params))\n",
    "    \n",
    "     # This calculates the percentage of correct values in a test set and a predicted set\n",
    "    def evaluate_acc(self, test_y, predicted_y):\n",
    "        return (np.sum(predicted_y == test_y) / len(test_y))\n",
    "\n",
    "# Adding potential values for iterations of gradient descent for grid search hyperparamter tuning later\n",
    "iterations= []\n",
    "for i in range(1,11):\n",
    "    iterations.append(10*i)\n",
    "\n",
    "# The following iteration list was used for the mpg data set since it requires more iterations for higher accuracy\n",
    "# for i in range(10,100):\n",
    "#     iterations.append(10*i)\n",
    "\n",
    "# Adding potential values for learning rate into a list for grid search hyperparamter tuning later\n",
    "learning_rate= [0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009,\n",
    "               0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,\n",
    "               0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,\n",
    "               0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "# Instantiated the model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "'''\n",
    "The following commented out code are sequences of X and y arrays which correspond to\n",
    "the features and then the classifying outputs of the data sets.\n",
    "The first one will initialise the numpy arrays for X and y for the adult income data set.\n",
    "The second one will initialise the numpy arrays for X and y for the ionosphere data set.\n",
    "The third one will initialise the numpy arrays for X and y for the MPG data set.\n",
    "The fourth one will initialise the numpy arrays for X and y for the breast cancer data set.\n",
    "'''\n",
    "# features = df.drop(['salary'] , axis=1)\n",
    "# X = features.values\n",
    "# output = df['salary']\n",
    "# y = output.values\n",
    "\n",
    "# features = df_ion.drop(['classification'] , axis=1)\n",
    "# X = features.values\n",
    "# output = df_ion['classification']\n",
    "# y = output.values\n",
    "\n",
    "# features = DataFrame.drop(['mpg'] , axis=1)\n",
    "# X = features.values\n",
    "# output = DataFrame['mpg']\n",
    "# y = output.values\n",
    "\n",
    "features = df_bcw.drop(['Class'] , axis=1)\n",
    "X = features.values\n",
    "output = df_bcw['Class']\n",
    "y = output.values\n",
    "\n",
    "\n",
    "# The following plots the dataset\n",
    "# sns.set_style('white')\n",
    "# fig = sns.scatterplot(X[:,0],X[:,1],hue=y.reshape(-1));\n",
    "\n",
    "y = y[:,np.newaxis]\n",
    "m = len(y)\n",
    "X = np.hstack((np.ones((m,1)),X))\n",
    "n = np.size(X,1)\n",
    "params = np.zeros((n,1))\n",
    "\n",
    "# Hyperparameter tuning using grid search\n",
    "optimal_score = 0\n",
    "optimal_rate = 0\n",
    "optimal_iter = 0\n",
    " # Goes through every possible combination in the grid for the learning_rate and iterations and tests to find the \n",
    " # highest \n",
    "for rate in learning_rate:\n",
    "    for iter in iterations:\n",
    "        score = kFoldCrossVal(5, X, y, rate, iter)\n",
    "        if score > optimal_score:\n",
    "            optimal_score = score\n",
    "            optimal_rate = rate\n",
    "            optimal_iter = iter\n",
    "\n",
    "print(\"Optimal score is {}, with rate: {} and {} iterations\".format(optimal_score, optimal_rate, optimal_iter))\n",
    "\n",
    "'''\n",
    "The following tests accuracy of the data against the size of the data set. \n",
    "It splits the dataset into tenths, and then every iteration joins a tenth together, and\n",
    "finally tests it for the whole dataset for the tenth iteration.\n",
    "'''\n",
    "# accuracy_vals = []\n",
    "# size_of_dataset = []\n",
    "# for i in range(1,11):\n",
    "#     feats = X[:int(i*(len(y)/10))]\n",
    "#     outputs = y[:int(i*(len(y)/10))]\n",
    "#     accuracy = kFoldCrossVal(5, feats, outputs, optimal_rate, optimal_iter )\n",
    "#     print(accuracy, len(outputs))\n",
    "#     accuracy_vals.append(accuracy)\n",
    "#     size_of_dataset.append(len(outputs))\n",
    "    \n",
    "# plt.plot(size_of_dataset,accuracy_vals,'-ro')\n",
    "# plt.xlabel('Size of the Data Set')\n",
    "# plt.ylabel('Accuracy Values')\n",
    "# # plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "The following are the experiments ran for valuating learning rate against the accuracy. Based on the \n",
    "optimal_iterations found in the previous hyperparameter tuning, these functions evaluate the range of \n",
    "the learning rate, and plot its accuracy for that range.\n",
    "'''\n",
    "learning_rates0 =  [0.00001,0.00002,0.00003,0.00004,0.00005,0.00006,0.00007,0.00008,0.00009]\n",
    "learning_rates1 =  [0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009]\n",
    "learning_rates2 = [0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009]\n",
    "learning_rates3 = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09]\n",
    "learning_rates4 = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "\n",
    "####### plotting learning rate values in the (0.00001's) #######\n",
    "\n",
    "# accuracy_values0 = []\n",
    "# for rate in learning_rates1:\n",
    "#     accuracy = kFoldCrossVal(5, X, y, rate, optimal_iter)\n",
    "#     accuracy_values0.append(accuracy)\n",
    "\n",
    "# plt.plot(learning_rates0, accuracy_values0)\n",
    "# plt.title('Accuracy of Learning rates in 0.0001s ')\n",
    "# plt.xlabel('Learning Rates')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.show()\n",
    "\n",
    "###### plotting learning rate values in the (0.0001's) #######\n",
    "# accuracy_values1 = []\n",
    "# for rate in learning_rates1:\n",
    "#     accuracy = kFoldCrossVal(5, X, y, rate, optimal_iter)\n",
    "#     accuracy_values1.append(accuracy)\n",
    "\n",
    "# plt.plot(learning_rates1, accuracy_values1)\n",
    "# plt.title('Accuracy of Learning rates in 0.0001s ')\n",
    "# plt.xlabel('Learning Rates')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.show()\n",
    "\n",
    "# # ######## plotting learning rate values in the (0.001's) #######\n",
    "# accuracy_values2 = []\n",
    "# for rate in learning_rates2:\n",
    "#     accuracy = kFoldCrossVal(5, X, y, rate, optimal_iter)\n",
    "#     accuracy_values2.append(accuracy)\n",
    "    \n",
    "# plt.plot(learning_rates2, accuracy_values2)\n",
    "# plt.title('Accuracy of Learning rates in 0.001s ')\n",
    "# plt.xlabel('Learning Rates')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# ######## plotting learning rate values in the (0.01's)#######\n",
    "# # accuracy_values3 = []    \n",
    "# # for rate in learning_rates3:\n",
    "# #     accuracy = kFoldCrossVal(5, X, y, rate, optimal_iter)\n",
    "# #     accuracy_values3.append(accuracy)\n",
    "\n",
    "# # plt.plot(learning_rates3, accuracy_values3)\n",
    "# # plt.title('Accuracy of Learning rates in 0.01s ')\n",
    "# # plt.xlabel('Learning Rates')\n",
    "# # plt.ylabel('Accuracy')\n",
    "# # plt.show()\n",
    "    \n",
    "\n",
    "#### plotting accuracy on learning rate values in the (0.1's) #######\n",
    "# accuracy_values4 = []    \n",
    "# for rate in learning_rates4:\n",
    "#     #used 300 as it is the optimal \n",
    "#     accuracy = kFoldCrossVal(5, X, y, rate, optimal_iter)\n",
    "#     accuracy_values4.append(accuracy)\n",
    "\n",
    "# plt.plot(learning_rates4, accuracy_values4)\n",
    "# plt.title('Accuracy of Learning rates in 0.1s ')\n",
    "# plt.xlabel('Learning Rates')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
